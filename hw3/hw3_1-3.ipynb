{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab056799",
   "metadata": {},
   "source": [
    "## 1. (15 points) Backpropagation for autoencoders.\n",
    "***\n",
    "+ (a) As we can see in the loss function, the distance between $\\mathbf{W}^T\\mathbf{W}\\mathbf{x}$ and $\\mathbf{x}$ will be minimized. Therefore, the hidden vectors can represent the information about $\\mathbf{x}$.\n",
    "+ (b) <img src=\"1-b.jpg\"> \n",
    "+ (c) We simply add gradients from the two paths up. For example, there are two paths from $a$ to $d$: $a\\rightarrow b\\rightarrow d$ and $a\\rightarrow c\\rightarrow d$, then we have\n",
    "$$\n",
    "\\frac{\\partial d}{\\partial a}=\\frac{\\partial d}{\\partial b}\\cdot\\frac{\\partial b}{\\partial a}\n",
    "+\\frac{\\partial d}{\\partial c}\\cdot\\frac{\\partial c}{\\partial a}\n",
    "$$\n",
    "+ (d) First let $\\mathbf{D}=\\mathbf{W}^T\\mathbf{W}\\mathbf{x}-\\mathbf{x}$, we have\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{D}}=\\mathbf{D}\n",
    "$$\n",
    "Next backpropagate to $\\mathbf{W}$ and $\\mathbf{W}^T$, we have\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Wx}}=\\mathbf{WD}\n",
    "&\\Rightarrow \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}}=\\mathbf{W}\\mathbf{D}\\mathbf{x}^T \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W^T}}&=\\mathbf{D}(\\mathbf{Wx})^T\n",
    "\\end{aligned}\n",
    "$$\n",
    "Finally add the above two terms up, we have\n",
    "$$\n",
    "\\nabla_\\mathbf{W}\\mathcal{L}=\\mathbf{W}\\mathbf{D}\\mathbf{x}^T+\\big(\\mathbf{D}(\\mathbf{Wx})^T\\big)^T=\\mathbf{W}\\mathbf{D}\\mathbf{x}^T+\\mathbf{WxD}^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b1f204",
   "metadata": {},
   "source": [
    "### 2. (20 points) Backpropagation for Gaussian-process latent variable model.\n",
    "***\n",
    "+ (a) <img src=\"2-a.jpg\"> \n",
    "+ (b) \n",
    "$$\n",
    "\\frac{\\partial{\\mathcal{L}_1}}{\\partial\\mathbf{K}}=-\\frac{D}{2}(\\mathbf{K}^{-1})^T\n",
    "\\Rightarrow \\frac{\\partial{\\mathcal{L}_1}}{\\partial\\mathbf{X}}=-\\alpha D(\\mathbf{K}^{-1})^T\\mathbf{X}\n",
    "$$\n",
    "+ (c) <img src=\"2-c.jpg\"> \n",
    "+ (d)\n",
    "$$\n",
    "\\frac{\\partial{\\mathcal{L}_2}}{\\partial\\mathbf{K}^{-1}\\mathbf{Y}\\mathbf{Y}^T}=-\\frac{1}{2}\\mathbf{I}\n",
    "\\Rightarrow \\frac{\\partial{\\mathcal{L}_2}}{\\partial\\mathbf{K}^{-1}}=-\\frac{1}{2}\\mathbf{Y}\\mathbf{Y}^T\n",
    "\\Rightarrow \\frac{\\partial{\\mathcal{L}_2}}{\\partial\\mathbf{K}}=\\frac{1}{2}\\mathbf{K}^{-1}\\mathbf{Y}\\mathbf{Y}^T\\mathbf{K}^{-1}\n",
    "$$\n",
    "From (b), we have\n",
    "$$\n",
    "\\frac{\\partial{\\mathcal{L}_2}}{\\partial\\mathbf{X}}=\\alpha\\mathbf{K}^{-1}\\mathbf{Y}\\mathbf{Y}^T\\mathbf{K}^{-1}\\mathbf{X}\n",
    "$$\n",
    "+ (e) Add the results from (b) and (d) up, we have\n",
    "$$\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{X}}=-\\alpha D(\\mathbf{K}^{-1})^T\\mathbf{X}+\\alpha\\mathbf{K}^{-1}\\mathbf{Y}\\mathbf{Y}^T\\mathbf{K}^{-1}\\mathbf{X}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b355d30b",
   "metadata": {},
   "source": [
    "### 3. (15 points) NNDL to the rescue!!\n",
    "***\n",
    "+ (a) <img src=\"3-a.jpg\"> \n",
    "+ (b) First formulate cross entropy loss\n",
    "$$\n",
    "L=-\\log\\text{softmax}_y(h_1)\n",
    "=-\\log\\frac{\\exp z_{2,y}}{\\sum_{k=1}^C\\exp z_{2,k}}\n",
    "$$\n",
    "Next backpropagate to $z_2$, we have\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_2}=-(\\mathbb{1}_y-p)\n",
    "$$\n",
    "where $\\mathbb{1}_y$ is a vector with only $y$-th element equal to $1$ and $0$ otherwise and\n",
    "$$\n",
    "p=\\begin{bmatrix}\n",
    "\\frac{\\exp z_{2,1}}{\\sum_{k=1}^C\\exp z_{2,k}}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\exp z_{2,C}}{\\sum_{k=1}^C\\exp z_{2,k}}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Finally backpropagate to $W_2$ and $b_2$, we have\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_2}=\\frac{\\partial L}{\\partial z_2}\\cdot h_1^T\\text{ and }\\frac{\\partial L}{\\partial b_2}=\\frac{\\partial L}{\\partial z_2}\n",
    "$$\n",
    "+ (c) By the result of (b), we have\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h_1}=W_2^T\\cdot\\frac{\\partial L}{\\partial z_2}\n",
    "$$\n",
    "Next backpropagate to $z_1$, we have\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_1}=\\frac{\\partial L}{\\partial h_1}\\odot\\frac{\\partial h_1}{\\partial z_1}=\\frac{\\partial L}{\\partial h_1}\\odot \\big(\\sigma(z_1)+z_1\\odot\\sigma(z_1)\\odot(1-\\sigma(z_1))\\big)\n",
    "$$\n",
    "Let\n",
    "$$\n",
    "p=\\frac{\\partial L}{\\partial z_1}=\\big(W_2^T\\cdot\\frac{\\partial L}{\\partial z_2}\\big)\\odot \\big(\\sigma(z_1)+z_1\\odot\\sigma(z_1)\\odot(1-\\sigma(z_1))\\big)\n",
    "$$\n",
    "Finally backpropagate to $W_1$ and $b_1$, we have\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_1}=p\\cdot x^T\\text{ and }\n",
    "\\frac{\\partial L}{\\partial b_1}=p\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
