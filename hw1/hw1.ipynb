{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e1d7f02",
   "metadata": {},
   "source": [
    "# ECE C247 HW1\n",
    "***\n",
    "+ Name: Kuei-Chun Huang\n",
    "+ UID: 705668757"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d3902c",
   "metadata": {},
   "source": [
    "### 1. (25 points) Linear algebra refresher.\n",
    "***\n",
    "+ (a) (12 points) Let $Q$ be a real orthogonal matrix.\n",
    "    + i. (3 points) Show that $Q^T$ and $Q^{−1}$ are also orthogonal.\n",
    "        + **Ans: Since $Q$ is orthogonal, $Q^TQ=QQ^T=I$ and $Q^T=Q^{−1}$. Therefore, both $Q^T$ and $Q^{−1}$ are orthogonal.**\n",
    "    + ii. (3 points) Show that $Q$ has eigenvalues with norm 1.\n",
    "        + **Ans: Let $\\lambda$ be an eigenvalue of $Q$ and let $v$ be a corresponding eigenvector, we have**\n",
    "        $$\n",
    "        \\lVert Qv \\rVert^2=v^TQ^TQv=v^TIv=\\lVert v \\rVert^2=\\lambda^2\\lVert v \\rVert^2\\Rightarrow |\\lambda|=1\n",
    "        $$\n",
    "    + iii. (3 points) Show that the determinant of $Q$ is either +1 or -1.\n",
    "        + **Ans: Since $Q^TQ=I$, we have**\n",
    "        $$\n",
    "        \\det(Q^TQ)=\\det(Q^T)\\det(Q)=(\\det(Q))^2=\\det(I)=1\\Rightarrow \\det(Q)=1\\text{ or }-1\n",
    "        $$\n",
    "    + iv. (3 points) Show that $Q$ defines a length preserving transformation.\n",
    "        + **Ans: Let $v$ be a vector**\n",
    "        $$\n",
    "        \\lVert Qv \\rVert^2=v^TQ^TQv=v^TIv=\\lVert v \\rVert^2\n",
    "        $$\n",
    "+ (b) (8 points) Let $A$ be a matrix.\n",
    "    + i. (4 points) What is the relationship between the singular vectors of $A$ and the eigenvectors of $AA^T$? What about $A^TA$?\n",
    "        + **Ans: The eigenvectors of $AA^T$ are the left singular vectors of $A$, while the eigenvectors of $A^TA$ are the right singular vectors of $A$.**\n",
    "    + ii. (4 points) What is the relationship between the singular values of $A$ and the eigenvalues of $AA^T$? What about $A^TA$?\n",
    "        + **Ans: The singular values of $A$ are square roots of eigenvalues from $AA^T$ or $A^TA$.**\n",
    "+ (c) (5 points) True or False. Partial credit on an incorrect solution may be awarded if you justify your answer.\n",
    "    + i. [**False**] Every linear operator in an n-dimensional vector space has n distinct eigenvalues.\n",
    "    + ii. [**False**] A non-zero sum of two eigenvectors of a matrix $A$ is an eigenvector.\n",
    "    + iii. [**True**] If a matrix $A$ has the positive semidefinite property, i.e., $x^T Ax ≥ 0$ for all $x$, then its eigenvalues must be non-negative.\n",
    "    + iv. [**True**] The rank of a matrix can exceed the number of distinct non-zero eigenvalues.\n",
    "    + v. [**True**] A non-zero sum of two eigenvectors of a matrix $A$ corresponding to the same eigenvalue $\\lambda$ is always an eigenvector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f98aec4",
   "metadata": {},
   "source": [
    "### 2. (25 points) Probability refresher.\n",
    "***\n",
    "+ (a) (5 points) A jar of coins is equally populated with two types of coins. One is type “H50” and comes up heads with probability 0.5. Another is type “H60” and comes up heads with probability 0.6.\n",
    "    + i. (1 points) You take one coin from the jar and flip it. It lands tails. What is the posterior probability that this is an H50 coin?\n",
    "        + **Ans: $\\frac{0.5}{0.5+0.4}\\approx0.56$**\n",
    "    + ii. (2 points) You put the coin back, take another, and flip it 4 times. It lands T, H, H, H. How likely is the coin to be type H50?\n",
    "        + **Ans: $\\frac{0.5^4}{0.5^4+0.4\\cdot0.6^3}\\approx0.42$**\n",
    "    + iii. (2 points) A new jar is now equally populated with coins of type H50, H55, and H60 (with probabilities of coming up heads 0.5, 0.55, and 0.6 respectively). You take one coin and flip it 10 times. It lands heads 9 times. How likely is the coin to be of each possible type?\n",
    "        + **Ans:**\n",
    "        $$\n",
    "        \\begin{aligned}\n",
    "        P(H50)&=\\frac{0.5^{10}}{0.5^{10}+0.55^9\\cdot0.45+0.6^9\\cdot0.4}\\approx0.14 \\\\\n",
    "        P(H55)&=\\frac{0.55^9\\cdot0.45}{0.5^{10}+0.55^9\\cdot0.45+0.6^9\\cdot0.4}\\approx0.29 \\\\\n",
    "        P(H60)&=\\frac{0.6^9\\cdot0.4}{0.5^{10}+0.55^9\\cdot0.45+0.6^9\\cdot0.4}\\approx0.57\n",
    "        \\end{aligned}\n",
    "        $$\n",
    "+ (b) (5 points) Students at UCLA are from these disciplines: 15% Science, 21% Healthcare, 24% Liberal Arts, and 40% Engineering. (Each student belongs to a unique discipline.) The students attend a lecture and give feedback. Suppose 90% of the Science students liked the lecture, 18% of the Healthcare students liked it, none of the Liberal Arts students liked it, and 10% of the Engineering students liked it. If a student is randomly chosen, and the student liked the lecture, what is the conditional probability that the student is from Science?\n",
    "    + **Ans: $\\frac{0.15\\cdot0.9}{0.15\\cdot0.9+0.21\\cdot0.18+0.24\\cdot0+0.4\\cdot0.1}\\approx0.63$**\n",
    "+ (c) (5 points) Consider a pregnancy test with the following statistics.\n",
    "• If the woman is pregnant, the test returns “positive” (or 1, indicating the woman\n",
    "is pregnant) 99% of the time.\n",
    "• If the woman is not pregnant, the test returns “positive” 10% of the time.\n",
    "• At any given point in time, 99% of the female population is not pregnant.\n",
    "What is the probability that a woman is pregnant given she received a positive test?\n",
    "The answer should make intuitive sense; give an explanation of the result that you find.\n",
    "    + **Ans: The probability will be $\\frac{0.01\\cdot0.99}{0.99\\cdot0.1+0.01\\cdot0.99}\\approx0.09$. Since most of the women are not pregnant and the false-positive rate is not low, the probability that a woman is pregnant should be low even though she received a positive result.**\n",
    "+ (d) (5 points) Let $x_1, x_2, \\cdots , x_n$ be identically distributed random variables. A random vector, $x$, is defined as\n",
    "$$\n",
    "x=\\begin{bmatrix}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "\\vdots\\\\\n",
    "x_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "What is $E (Ax + b)$ in terms of $E(x)$, given that $A$ and $b$ are deterministic?\n",
    "    + **Ans: Let\n",
    "    $$\n",
    "    A \\in R^{m×n}=\\begin{bmatrix}a_1^T\\\\\\vdots\\\\a_m^T\\end{bmatrix}, b \\in R^m\n",
    "    $$\n",
    "    we have**\n",
    "    $$\n",
    "    Ax+b = \\begin{bmatrix}a_1^Tx+b_1\\\\\\vdots\\\\a_m^Tx+b_m\\end{bmatrix} \\\\\n",
    "    \\Rightarrow E(Ax+b)=\\begin{bmatrix}E(a_1^Tx+b_1)\\\\\\vdots\\\\E(a_m^Tx+b_m)\\end{bmatrix}\n",
    "    =\\begin{bmatrix}a_1^TE(x)+b_1\\\\\\vdots\\\\a_m^TE(x)+b_m\\end{bmatrix}\n",
    "    =AE(x)+b\n",
    "    $$\n",
    "+ (d) (5 points) Let\n",
    "$$\n",
    "\\text{cov}(x) = E\\big((x − E(x))(x − E(x))^T\\big)\n",
    "$$\n",
    "What is $\\text{cov}(Ax + b)$ in terms of $\\text{cov}(x)$, given that $A$ and $b$ are deterministic?\n",
    "    + **Ans:**\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\text{cov}(Ax + b)&= E\\big((Ax+b − E(Ax+b))(Ax+b − E(Ax+b))^T\\big)\\\\\n",
    "    &=E\\big(A(x − E(x))(x − E(x))^TA^T\\big)\\\\\n",
    "    &=AE\\big((x − E(x))(x − E(x))^T\\big)A^T\\\\\n",
    "    &=A\\text{cov}(x)A^T\n",
    "    \\end{aligned}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e124e7fe",
   "metadata": {},
   "source": [
    "### 3. (10 points) Multivariate derivatives.\n",
    "***\n",
    "+ (a) (1 points) Let $x \\in R^n, y \\in R^m$, and $A \\in R^{n×m}$. What is $∇_xx^T Ay$?\n",
    "    + **Ans: $Ay$**\n",
    "+ (b) (1 points) Let $x \\in R^n, y \\in R^m$, and $A \\in R^{n×m}$. What is $∇_yx^T Ay$?\n",
    "    + **Ans: $A^Tx$**\n",
    "+ (c) (1 points) Let $x \\in R^n, y \\in R^m$, and $A \\in R^{n×m}$. What is $∇_Ax^T Ay$?\n",
    "    + **Ans: $xy^T$**\n",
    "+ (d) (1 points) Let $x \\in R^n, b \\in R^n$, $A \\in R^{n×n}$, and let $f=x^TAx+b^Tx$. What is $∇_xf$?\n",
    "    + **Ans: $(A+A^T)x+b$**\n",
    "+ (e) (1 points) Let $A \\in R^{n×n}, B \\in R^{n×n}$, and $f = \\text{tr}(AB)$. What is $∇_Af$?\n",
    "    + **Ans: $B^T$**\n",
    "+ (f) (1 points) Let $A \\in R^{n×n}, B \\in R^{n×n}$, and $f = \\text{tr}(BA+A^TB+A^2B)$. What is $∇_Af$?\n",
    "    + **Ans: $B^T+B+(AB+BA)^T$**\n",
    "+ (g) (1 points) Let $A \\in R^{n×n}, B \\in R^{n×n}$, and $f = \\lVert A+\\lambda B \\rVert^2_F$. What is $∇_Af$?\n",
    "    + **Ans: $f=\\text{tr}\\big((A+\\lambda B)(A+\\lambda B)^T\\big)=\\text{tr}(AA^T+\\lambda BA^T+\\lambda AB^T+\\lambda^2BB^T)\\\\ \\Rightarrow ∇_Af=2(A+\\lambda B)$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f48a33",
   "metadata": {},
   "source": [
    "### 4. (10 points) Deriving least-squares with matrix derivatives.\n",
    "***\n",
    "+ In least-squares, we seek to estimate some multivariate output $y$ via the model\n",
    "$$\n",
    "\\hat{y} = Wx\n",
    "$$\n",
    "In the training set we’re given paired data examples $(x^{(i)}, y^{(i)})$ from $i = 1, \\cdots , n$. Least-squares is the following quadratic optimization problem:\n",
    "$$\n",
    "\\min_W\\frac{1}{2}\\sum_{i=1}^n\\lVert y^{(i)}-Wx^{(i)}\\rVert^2\n",
    "$$\n",
    "Derive the optimal $W$.\n",
    "Where $W$ is a matrix, and for each example in the training set, both $x^{(i)}$ and $y^{(i)}, ∀i = 1, \\cdots , n$ are vectors.\n",
    "Hint: you may find the following derivatives useful:\n",
    "$$\n",
    "\\frac{∂\\text{tr}(WA)}{∂W} = A^T\n",
    "$$\n",
    "$$\n",
    "\\frac{∂\\text{tr}(WAW^T)}{∂W} = WA^T + WA\n",
    "$$\n",
    "    + **Ans: First re-write the loss function\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    L(W)=\\frac{1}{2}\\sum_{i=1}^n\\lVert y^{(i)}-Wx^{(i)}\\rVert^2\n",
    "    &=\\frac{1}{2}\\sum_{i=1}^n(y^{(i)}-Wx^{(i)})^T(y^{(i)}-Wx^{(i)}) \\\\\n",
    "    &=\\frac{1}{2}\\sum_{i=1}^n( y^{(i)T}y^{(i)} -2y^{(i)T}Wx^{(i)}+x^{(i)T}W^TWx^{(i)}) \\\\\n",
    "    &=\\frac{1}{2}\\sum_{i=1}^n\\big(\\text{tr}(y^{(i)T}y^{(i)}) -2\\text{tr}(y^{(i)T}Wx^{(i)})+\\text{tr}(x^{(i)T}W^TWx^{(i)})\\big) \\\\\n",
    "    &=\\frac{1}{2}\\big(\\text{tr}(Y^TY)-2\\text{tr}(Y^TWX)+\\text{tr}(X^TW^TWX)\\big) \\\\\n",
    "    &=\\frac{1}{2}\\big(\\text{tr}(Y^TY)-2\\text{tr}(WXY^T)+\\text{tr}(WXX^TW^T)\\big)\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "    Then take derivative\n",
    "    $$\n",
    "    ∇_WL(W)=\\frac{1}{2}(0-2YX^T+2WXX^T)=WXX^T-YX^T\n",
    "    $$\n",
    "    Finally set the derivative to 0 to minimize it, we have**\n",
    "    $$\n",
    "    W^*=YX^T(XX^T)^{-1}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c572f5fc",
   "metadata": {},
   "source": [
    "### 5. (10 points) Regularized least squares.\n",
    "***\n",
    "+ In lecture, we worked through the following least squares problem\n",
    "$$\n",
    "\\arg\\min_\\theta\\frac{1}{2}\\sum_{i=1}^n (y^{(i)}-\\theta^T\\hat{x}^{(i)})^2\n",
    "$$\n",
    "However, the least squares has a tendency to overfit the training data. One common technique used to address the overfitting problem is regularization. In this problem, we work through one of the regularization techniques namely ridge regularization which is also known as the regularized least squares problem. In the regularized least squares we solve the following optimization problem\n",
    "$$\n",
    "\\arg\\min_\\theta\\frac{1}{2}\\sum_{i=1}^n (y^{(i)}-\\theta^T\\hat{x}^{(i)})^2\n",
    "+\\frac{\\lambda}{2}\\lVert\\theta\\rVert^2_2\n",
    "$$\n",
    "where $λ$ is a tunable regularization parameter. From the above cost function it can be observed that we are seeking least squares solution with a smaller 2-norm. Derive the solution to the regularized least squares problem, i.e Find $θ^*$.\n",
    "    + **Ans: By using the derivative from class, we have the derivative of the regularized loss function\n",
    "    $$\n",
    "    X^TX\\theta-X^TY+\\frac{\\lambda}{2}∇_\\theta\\lVert\\theta\\rVert^2_2\n",
    "    =X^TX\\theta-X^TY+\\lambda\\theta\n",
    "    $$\n",
    "    then set the derivative to 0 to minimize it, we have**\n",
    "    $$\n",
    "    \\theta^*=(X^TX+\\lambda I)^{-1}X^TY\n",
    "    $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
